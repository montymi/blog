[
    {
	"id": 3,
	"title": "Talking Docs",
	"content": "I am currently working to add audio files to the releases within Discography. The walkthroughs will answer W5H in an experimental documentation technique. The podcast-like approach hopes to reduce the friction point of using new software, either as a consumer or a developer, by offering a more digestible introduction to the system's purpose and history.",
	"date": "2024-12-01"
    },   
    {
        "id": 2,
        "title": "Technology Beyond the Keyboard: The Evolution of Voice Interaction",
        "content": "Whenever technology breaks free from its traditional interfaces, the disruption in the market is profound. Take, for example, the advent of voice assistants—Alexa, initially, brought home automation and rudimentary language processing into the mainstream. But the evolution didn't stop there. Fast forward to today, and companies like Spotify are innovating to make podcasts accessible in multiple languages by creating a sophisticated Speech-to-Text (STS) translation tool. This marks a monumental leap in how we interact with content and how technology speaks our language. OpenAI’s open-source release of their Whisper model for speech-to-text took the next giant step, putting local speech recognition within anyone’s reach with just a simple import. It’s remarkable that the completion of the local speech pipeline now feels so… attainable. But there's still one crucial piece left: Text-to-Speech (TTS). After digging through various tools and libraries, I stumbled upon Coqui TTS, a robust, multi-lingual speech synthesis library capable of cloning voices from audio files. The project stands out for its focus on fine-tuning voice models, allowing for more personalized interaction with machines. While the results are still developing, especially on older hardware like my 2019 Macbook, it’s clear that TTS is catching up to the Turing-like promise of machines that “speak” to us, not just in terms of outputs but with personality and nuance. Together, STT and TTS create a seamless feedback loop that finally brings us closer to fully integrated communication systems. Add a language model (LLM) for cognitive processing, and you have yourself a brainstorming partner—a technological avatar capable of multi-lingual thought, communication, and creativity. We’re witnessing the gradual collapse of the barriers between human cognition and machine understanding. And in this space, the blending of linguistic models, speech recognition, and artificial intelligence will enable us to reach deeper into how we think, speak, and create. As I explore these tools, I can’t help but wonder: what’s next? Where will the convergence of voice technology, machine learning, and philosophy take us? This is just the beginning, and I feel like I’m witnessing something fundamental to the future of human interaction.",
        "date": "2024-11-25"
    },
    {
      "id": 1,
      "title": "Welcome to My Portfolio",
      "content": "This is the first blog post. Stay tuned for updates!",
      "date": "2024-11-25"
    }
]
  
